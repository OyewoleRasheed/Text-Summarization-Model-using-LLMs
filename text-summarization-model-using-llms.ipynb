{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"To build a text summarization model, first, we need to choose a pre-trained language model like T5. Then, we need to tokenize the input text, which converts it into a format the model can process. The next step will be to use the model to generate a summary by specifying parameters like maximum length and beam search for better results. The final step will be to decode the generated tokens back into readable text and adjust parameters to improve the summary quality.\n\nSo, let’s start building a text summarization model using LLMs with Python step by step.","metadata":{}},{"cell_type":"markdown","source":"So, let’s start building a text summarization model using LLMs with Python step by step.\n\n# Step 1: Select a Suitable LLM\nChoose a pre-trained model designed for text generation tasks. The T5 model (Text-to-Text Transfer Transformer) by Google is one such model that is effective for various text-based tasks like translation, question-answering, and summarization. \n\n# Step 2: Install Required Libraries\n\nInstall the transformers library by Hugging Face. It provides easy access to various pre-trained models and tokenizers. If you are using Google Colab, you will find it pre-installed in the Colab environment. To install it on your local machine, run the command mentioned below on your terminal or command prompt:","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T22:44:19.414662Z","iopub.execute_input":"2025-07-08T22:44:19.415026Z","iopub.status.idle":"2025-07-08T22:44:23.591739Z","shell.execute_reply.started":"2025-07-08T22:44:19.414994Z","shell.execute_reply":"2025-07-08T22:44:23.590462Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Step 3: Load the Pre-trained Model and Tokenizer\nNow, we need to import the T5Tokenizer and T5ForConditionalGeneration classes from the transformers library. Select a model like t5-small, t5-base, or t5-large based on the requirement and computational capacity:","metadata":{}},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n#load pre-trained T5  model and tokenizer\nmodel_name = 't5-small'\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T22:57:11.414629Z","iopub.execute_input":"2025-07-08T22:57:11.415130Z","iopub.status.idle":"2025-07-08T22:57:16.404169Z","shell.execute_reply.started":"2025-07-08T22:57:11.415092Z","shell.execute_reply":"2025-07-08T22:57:16.403150Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddf166d163d249d4a13a91108abb1f57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adf297df7f824be29d952960fbe26839"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17c13dc89eda4753a6e7fcd674520b39"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5026282664e4acba2aecdd4c94c6360"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"788206a1f66947bea838b08724e26cb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f43b17a60234a2e91a9c0dd802e22a7"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"To select between t5-small, t5-base, or t5-large, consider your computational resources and accuracy needs. t5-small is faster and requires less memory, which makes it suitable for quick tasks or limited hardware. t5-base offers a balance between speed and performance, ideal for general use. t5-large provides the highest accuracy but needs more memory and processing power, which makes it better for scenarios where performance is more important than speed.","metadata":{}},{"cell_type":"markdown","source":"# Step 4: Prepare the Text for Summarization\nNext, we need to define the text that needs to be summarized. The text should be prefixed with the keyword “summarize:” for the T5 model to recognize the task properly:","metadata":{}},{"cell_type":"code","source":"text = \"\"\"The COVID19 pandemic has brought unprecedented challenges to the global economy....\"\"\"\n#prepare the text for the t5 model by adding the summarize prefix\ninput_text = \"summarize: \" + text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T22:59:49.868695Z","iopub.execute_input":"2025-07-08T22:59:49.869153Z","iopub.status.idle":"2025-07-08T22:59:49.874822Z","shell.execute_reply.started":"2025-07-08T22:59:49.869120Z","shell.execute_reply":"2025-07-08T22:59:49.873534Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"The “summarize:” prefix for the T5 model is necessary because T5 is a “text-to-text” model that needs to understand what task it should perform (e.g., summarization, translation, or question-answering). The prefix helps the model identify that it should generate a summary of the input text. Without this instruction, the model might not produce the intended summarization output.","metadata":{}},{"cell_type":"markdown","source":"# Step 5: Tokenize the Input Text\nThe next step is tokenization. Tokenization is the process of converting text into a sequence of integers that represent the model’s vocabulary. The max_length parameter helps manage large texts by truncating or limiting the input size:","metadata":{}},{"cell_type":"code","source":"#tokenize the input text\ninput_ids = tokenizer.encode(input_text, return_tensors='pt',max_length=512,truncation = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T23:03:02.824223Z","iopub.execute_input":"2025-07-08T23:03:02.824630Z","iopub.status.idle":"2025-07-08T23:03:02.830449Z","shell.execute_reply.started":"2025-07-08T23:03:02.824603Z","shell.execute_reply":"2025-07-08T23:03:02.829449Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Step 6: Generate the Summary\nNow, use the generate method to produce the summary. Important parameters include:\n\nmax_length: The maximum number of tokens in the output.\nnum_beams: The number of beams for beam search (higher values improve results but increase computation).\nlength_penalty: Adjusts the length of the summary (penalizes lengthy outputs).","metadata":{}},{"cell_type":"code","source":"#generate the summary\nsummary_ids = model.generate(\n    input_ids,\n    max_length = 50, # maximum length of the summary\n    num_beams=4, # beam search for better results\n    length_penalty = 2.0,# length penalty to avoid lengthy summaries\n    early_stopping = True \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T23:09:26.066183Z","iopub.execute_input":"2025-07-08T23:09:26.066554Z","iopub.status.idle":"2025-07-08T23:09:27.026488Z","shell.execute_reply.started":"2025-07-08T23:09:26.066523Z","shell.execute_reply":"2025-07-08T23:09:27.025701Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Step 7: Decode and Display the Summary\nThe final step is to decode the generated summary using the tokenizer to convert the tokens back to readable text:","metadata":{}},{"cell_type":"code","source":"summary = tokenizer.decode(summary_ids[0], skip_special_tokens= True)\nprint(\"Summary:\", summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T23:10:51.526489Z","iopub.execute_input":"2025-07-08T23:10:51.526998Z","iopub.status.idle":"2025-07-08T23:10:51.538299Z","shell.execute_reply.started":"2025-07-08T23:10:51.526962Z","shell.execute_reply":"2025-07-08T23:10:51.537045Z"}},"outputs":[{"name":"stdout","text":"Summary: COVID19 pandemic has brought unprecedented challenges to the global economy. pandemic has brought unprecedented challenges to the global economy.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}